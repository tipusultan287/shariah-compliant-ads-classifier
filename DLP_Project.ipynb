{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0dG7pRIApFo"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout, BatchNormalization\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "\n",
        "IMAGE_WIDTH = 224\n",
        "IMAGE_HEIGHT = 224\n",
        "IMAGE_SIZE = (IMAGE_WIDTH, IMAGE_HEIGHT)\n",
        "IMAGE_CHANNELS = 3\n",
        "BATCH_SIZE = 32\n",
        "NUM_CLASSES = 1\n",
        "EPOCHS_HEAD_TRAINING = 15\n",
        "EPOCHS_FINE_TUNING = 30\n",
        "LEARNING_RATE_HEAD = 1e-3\n",
        "LEARNING_RATE_FINE_TUNE = 1e-5\n",
        "VAL_SPLIT = 0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('ads.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('unzipped')\n"
      ],
      "metadata": {
        "id": "tvzkxUORLsCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_DATA_PATH = '/content/unzipped/ads/ads'\n",
        "TRAIN_DIR = '/content/train'\n",
        "VAL_DIR = '/content/val'\n",
        "\n",
        "def create_train_val_dirs(base_path, train_path, val_path, val_split=0.2):\n",
        "    if os.path.exists(train_path):\n",
        "        shutil.rmtree(train_path)\n",
        "    if os.path.exists(val_path):\n",
        "        shutil.rmtree(val_path)\n",
        "\n",
        "    os.makedirs(train_path, exist_ok=True)\n",
        "    os.makedirs(val_path, exist_ok=True)\n",
        "\n",
        "    for class_name in os.listdir(base_path):\n",
        "        class_dir = os.path.join(base_path, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            os.makedirs(os.path.join(train_path, class_name), exist_ok=True)\n",
        "            os.makedirs(os.path.join(val_path, class_name), exist_ok=True)\n",
        "\n",
        "            images = [f for f in os.listdir(class_dir) if os.path.isfile(os.path.join(class_dir, f))]\n",
        "            random.shuffle(images) # Shuffle for random split\n",
        "\n",
        "            split_idx = int(len(images) * (1 - val_split))\n",
        "            train_images = images[:split_idx]\n",
        "            val_images = images[split_idx:]\n",
        "\n",
        "            for img_name in train_images:\n",
        "                shutil.copy(os.path.join(class_dir, img_name), os.path.join(train_path, class_name, img_name))\n",
        "            for img_name in val_images:\n",
        "                shutil.copy(os.path.join(class_dir, img_name), os.path.join(val_path, class_name, img_name))\n",
        "    print(f\"Created train and validation directories at {train_path} and {val_path}\")"
      ],
      "metadata": {
        "id": "CsFsXgyfNee-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_train_val_dirs(BASE_DATA_PATH, TRAIN_DIR, VAL_DIR, val_split=VAL_SPLIT)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ePyN1AZsQSXc",
        "outputId": "b049f1b2-86e8-40b3-f193-eb251c229e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created train and validation directories at /content/train and /content/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
        "\n",
        "# Training data generator with augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,\n",
        "    rotation_range=30,          # Randomly rotate images\n",
        "    width_shift_range=0.2,      # Randomly shift images horizontally\n",
        "    height_shift_range=0.2,     # Randomly shift images vertically\n",
        "    shear_range=0.2,            # Shear Intensity\n",
        "    zoom_range=0.2,             # Randomly zoom image\n",
        "    horizontal_flip=True,       # Randomly flip images horizontally\n",
        "    fill_mode='nearest'         # Strategy for filling newly created pixels\n",
        ")\n",
        "\n",
        "# Validation data generator (only rescaling/preprocessing, no augmentation)\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input\n",
        ")\n",
        "\n",
        "# Flow data from directories\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    TRAIN_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary', # For binary classification\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    VAL_DIR,\n",
        "    target_size=IMAGE_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False # No need to shuffle validation data\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5b84usBTwVN",
        "outputId": "1f94d4ac-529e-45cd-ed69-3a52129c857e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5283 images belonging to 2 classes.\n",
            "Found 1322 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = ResNet50(weights='imagenet', include_top=False,\n",
        "                      input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_CHANNELS))\n",
        "\n",
        "# Freeze the layers of the base model initially\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add custom classification head\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x) # Important to reduce dimensions\n",
        "x = Dense(512, activation='relu')(x)\n",
        "x = BatchNormalization()(x) # Helps stabilize training\n",
        "x = Dropout(0.5)(x)         # Regularization\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "x = Dropout(0.3)(x)\n",
        "predictions = Dense(NUM_CLASSES, activation='sigmoid')(x) # Sigmoid for binary\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4PM1xkdTzCp",
        "outputId": "d9d203a2-5d9e-4cf1-c650-4c0285e388e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94765736/94765736\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Training the classification head ---\")\n",
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_HEAD),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)\n",
        "model_checkpoint_head = ModelCheckpoint('best_model_head.keras', save_best_only=True, monitor='val_loss') # .keras extension\n",
        "reduce_lr_head = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6, verbose=1)\n",
        "\n",
        "history_head = model.fit(\n",
        "    train_generator,\n",
        "    #steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS_HEAD_TRAINING,\n",
        "    validation_data=validation_generator,\n",
        "    #validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    callbacks=[early_stopping, model_checkpoint_head, reduce_lr_head]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQ05hxOrT7My",
        "outputId": "3fc999cb-3abd-4c9d-fa8c-dedd51fcff23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Training the classification head ---\n",
            "Epoch 1/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 570ms/step - accuracy: 0.8502 - loss: 0.3395 - val_accuracy: 0.8530 - val_loss: 0.3490 - learning_rate: 0.0010\n",
            "Epoch 2/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 514ms/step - accuracy: 0.8591 - loss: 0.3404 - val_accuracy: 0.8568 - val_loss: 0.3200 - learning_rate: 0.0010\n",
            "Epoch 3/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 526ms/step - accuracy: 0.8544 - loss: 0.3336 - val_accuracy: 0.8644 - val_loss: 0.3272 - learning_rate: 0.0010\n",
            "Epoch 4/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 512ms/step - accuracy: 0.8597 - loss: 0.3243 - val_accuracy: 0.8682 - val_loss: 0.3077 - learning_rate: 0.0010\n",
            "Epoch 5/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 499ms/step - accuracy: 0.8640 - loss: 0.3117 - val_accuracy: 0.8652 - val_loss: 0.3114 - learning_rate: 0.0010\n",
            "Epoch 6/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 503ms/step - accuracy: 0.8731 - loss: 0.3004 - val_accuracy: 0.8652 - val_loss: 0.3131 - learning_rate: 0.0010\n",
            "Epoch 7/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - accuracy: 0.8687 - loss: 0.2993\n",
            "Epoch 7: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 510ms/step - accuracy: 0.8687 - loss: 0.2993 - val_accuracy: 0.8576 - val_loss: 0.3138 - learning_rate: 0.0010\n",
            "Epoch 8/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 501ms/step - accuracy: 0.8796 - loss: 0.2848 - val_accuracy: 0.8712 - val_loss: 0.3051 - learning_rate: 2.0000e-04\n",
            "Epoch 9/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 502ms/step - accuracy: 0.8832 - loss: 0.2727 - val_accuracy: 0.8682 - val_loss: 0.3094 - learning_rate: 2.0000e-04\n",
            "Epoch 10/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 504ms/step - accuracy: 0.8886 - loss: 0.2712 - val_accuracy: 0.8712 - val_loss: 0.3087 - learning_rate: 2.0000e-04\n",
            "Epoch 11/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 465ms/step - accuracy: 0.8904 - loss: 0.2517\n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 527ms/step - accuracy: 0.8903 - loss: 0.2518 - val_accuracy: 0.8674 - val_loss: 0.3118 - learning_rate: 2.0000e-04\n",
            "Epoch 12/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 531ms/step - accuracy: 0.8897 - loss: 0.2605 - val_accuracy: 0.8667 - val_loss: 0.3099 - learning_rate: 4.0000e-05\n",
            "Epoch 13/15\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 525ms/step - accuracy: 0.8926 - loss: 0.2610 - val_accuracy: 0.8659 - val_loss: 0.3098 - learning_rate: 4.0000e-05\n",
            "Epoch 13: early stopping\n",
            "Restoring model weights from the end of the best epoch: 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "print(f\"Number of layers in base_model: {len(base_model.layers)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W81KKWvRdaJH",
        "outputId": "98bcc518-0738-425a-92da-c59321a19888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of layers in base_model: 175\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=Adam(learning_rate=LEARNING_RATE_FINE_TUNE), # Crucial: very low LR\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Callbacks for fine-tuning\n",
        "model_checkpoint_fine_tune = ModelCheckpoint('best_model_fine_tuned.keras', save_best_only=True, monitor='val_loss') # .keras extension\n",
        "reduce_lr_fine_tune = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-7, verbose=1)\n",
        "# Early stopping can be more aggressive here or reuse the previous one if patience is suitable.\n",
        "early_stopping_ft = EarlyStopping(monitor='val_loss', patience=8, restore_best_weights=True, verbose=1)\n",
        "\n",
        "\n",
        "history_fine_tune = model.fit(\n",
        "    train_generator,\n",
        "    #steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
        "    epochs=EPOCHS_FINE_TUNING,\n",
        "    validation_data=validation_generator,\n",
        "    #validation_steps=validation_generator.samples // BATCH_SIZE,\n",
        "    callbacks=[early_stopping_ft, model_checkpoint_fine_tune, reduce_lr_fine_tune],\n",
        "    initial_epoch=history_head.epoch[-1] +1 if hasattr(history_head, 'epoch') and history_head.epoch else 0 # Continue epoch count\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVyQHvyxdbxN",
        "outputId": "c4fe4ea7-2ffc-4d67-eeda-74e3f74a239d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 777ms/step - accuracy: 0.8371 - loss: 0.3705 - val_accuracy: 0.8735 - val_loss: 0.3225 - learning_rate: 1.0000e-05\n",
            "Epoch 15/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 616ms/step - accuracy: 0.8708 - loss: 0.3044 - val_accuracy: 0.8682 - val_loss: 0.3179 - learning_rate: 1.0000e-05\n",
            "Epoch 16/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 653ms/step - accuracy: 0.8825 - loss: 0.2816 - val_accuracy: 0.8742 - val_loss: 0.3150 - learning_rate: 1.0000e-05\n",
            "Epoch 17/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 641ms/step - accuracy: 0.8876 - loss: 0.2648 - val_accuracy: 0.8750 - val_loss: 0.3132 - learning_rate: 1.0000e-05\n",
            "Epoch 18/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 702ms/step - accuracy: 0.8876 - loss: 0.2542 - val_accuracy: 0.8765 - val_loss: 0.3130 - learning_rate: 1.0000e-05\n",
            "Epoch 19/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m107s\u001b[0m 647ms/step - accuracy: 0.8956 - loss: 0.2367 - val_accuracy: 0.8780 - val_loss: 0.3020 - learning_rate: 1.0000e-05\n",
            "Epoch 20/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 688ms/step - accuracy: 0.9122 - loss: 0.2206 - val_accuracy: 0.8773 - val_loss: 0.2987 - learning_rate: 1.0000e-05\n",
            "Epoch 21/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 585ms/step - accuracy: 0.9209 - loss: 0.2022 - val_accuracy: 0.8773 - val_loss: 0.3052 - learning_rate: 1.0000e-05\n",
            "Epoch 22/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 585ms/step - accuracy: 0.9251 - loss: 0.1979 - val_accuracy: 0.8803 - val_loss: 0.3101 - learning_rate: 1.0000e-05\n",
            "Epoch 23/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 549ms/step - accuracy: 0.9383 - loss: 0.1704\n",
            "Epoch 23: ReduceLROnPlateau reducing learning rate to 1.9999999494757505e-06.\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 585ms/step - accuracy: 0.9383 - loss: 0.1705 - val_accuracy: 0.8795 - val_loss: 0.3042 - learning_rate: 1.0000e-05\n",
            "Epoch 24/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 577ms/step - accuracy: 0.9469 - loss: 0.1546 - val_accuracy: 0.8788 - val_loss: 0.3070 - learning_rate: 2.0000e-06\n",
            "Epoch 25/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 578ms/step - accuracy: 0.9406 - loss: 0.1552 - val_accuracy: 0.8803 - val_loss: 0.3059 - learning_rate: 2.0000e-06\n",
            "Epoch 26/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 556ms/step - accuracy: 0.9417 - loss: 0.1592\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 3.999999989900971e-07.\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 587ms/step - accuracy: 0.9417 - loss: 0.1592 - val_accuracy: 0.8803 - val_loss: 0.3040 - learning_rate: 2.0000e-06\n",
            "Epoch 27/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 578ms/step - accuracy: 0.9428 - loss: 0.1554 - val_accuracy: 0.8811 - val_loss: 0.3040 - learning_rate: 4.0000e-07\n",
            "Epoch 28/30\n",
            "\u001b[1m166/166\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 581ms/step - accuracy: 0.9495 - loss: 0.1528 - val_accuracy: 0.8803 - val_loss: 0.3031 - learning_rate: 4.0000e-07\n",
            "Epoch 28: early stopping\n",
            "Restoring model weights from the end of the best epoch: 20.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history_head, history_fine_tune=None, initial_epochs=0):\n",
        "    acc = history_head.history['accuracy']\n",
        "    val_acc = history_head.history['val_accuracy']\n",
        "    loss = history_head.history['loss']\n",
        "    val_loss = history_head.history['val_loss']\n",
        "\n",
        "    if history_fine_tune:\n",
        "        acc += history_fine_tune.history['accuracy']\n",
        "        val_acc += history_fine_tune.history['val_accuracy']\n",
        "        loss += history_fine_tune.history['loss']\n",
        "        val_loss += history_fine_tune.history['val_loss']\n",
        "\n",
        "    epochs_range_head = range(len(history_head.history['accuracy']))\n",
        "    total_epochs = len(acc)\n",
        "    epochs_range_total = range(total_epochs)\n",
        "\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range_total, acc, label='Training Accuracy')\n",
        "    plt.plot(epochs_range_total, val_acc, label='Validation Accuracy')\n",
        "    if history_fine_tune:\n",
        "        plt.plot([len(epochs_range_head)-1, len(epochs_range_head)-1],\n",
        "                 plt.ylim(), label='Start Fine-Tuning', linestyle='--')\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range_total, loss, label='Training Loss')\n",
        "    plt.plot(epochs_range_total, val_loss, label='Validation Loss')\n",
        "    if history_fine_tune:\n",
        "         plt.plot([len(epochs_range_head)-1, len(epochs_range_head)-1],\n",
        "                 plt.ylim(), label='Start Fine-Tuning', linestyle='--')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_history(history_head, history_fine_tune, initial_epochs=EPOCHS_HEAD_TRAINING)\n"
      ],
      "metadata": {
        "id": "6BwmsD8zkugM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nLoading best fine-tuned model for evaluation...\")\n",
        "best_model = tf.keras.models.load_model('best_model_fine_tuned.keras')\n",
        "\n",
        "# Evaluate on validation set (or a separate test set if you have one)\n",
        "eval_results = best_model.evaluate(validation_generator,\n",
        "                                   steps=validation_generator.samples // BATCH_SIZE,\n",
        "                                   verbose=1)\n",
        "print(f\"\\nBest Fine-tuned Model Validation Loss: {eval_results[0]:.4f}\")\n",
        "print(f\"Best Fine-tuned Model Validation Accuracy: {eval_results[1]:.4f}\")"
      ],
      "metadata": {
        "id": "5TsfBw5xlEeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "Y_pred_probs = best_model.predict(validation_generator, steps=validation_generator.samples // BATCH_SIZE +1) # Ensure all samples are predicted\n",
        "Y_pred = (Y_pred_probs > 0.5).astype(int) # Threshold at 0.5 for binary\n",
        "y_true = validation_generator.classes # True labels\n",
        "\n",
        "num_val_samples = validation_generator.samples\n",
        "if len(Y_pred) > num_val_samples:\n",
        "    Y_pred = Y_pred[:num_val_samples]\n",
        "\n",
        "print('\\nConfusion Matrix')\n",
        "cm = confusion_matrix(y_true, Y_pred)\n",
        "print(cm)\n",
        "\n",
        "print('\\nClassification Report')\n",
        "\n",
        "class_labels = list(validation_generator.class_indices.keys())\n",
        "print(classification_report(y_true, Y_pred, target_names=class_labels))"
      ],
      "metadata": {
        "id": "nIPUfdByltaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}